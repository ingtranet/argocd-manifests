apiVersion: apps/v1
kind: Deployment
metadata:
  name: embeddinggemma-300m
  namespace: llm-serving
  labels:
    app: embeddinggemma-300m
spec:
  replicas: 1
  selector:
    matchLabels:
      app: embeddinggemma-300m
  template:
    metadata:
      labels:
        app: embeddinggemma-300m
    spec:
      runtimeClassName: nvidia
      containers:
        - name: llama-cpp
          image: docker.io/cookieshake/llamacpp-jetson:b6941 
          imagePullPolicy: Always
          args:
            - --host
            - "0.0.0.0"
            - --port
            - "8080"
            - --parallel
            - "5"
            - --n-gpu-layers
            - "999"
            - --hf-repo
            - ggml-org/embeddinggemma-300m-qat-q8_0-GGUF:Q8_0
            - --embedding
            - --pooling
            - last
            - --ubatch-size
            - "8192"
            - --verbose-prompt
          ports:
            - containerPort: 8080
              name: http
          env:
            - name: LLAMA_CACHE
              value: /opt/llama_cpp/cache
          volumeMounts:
            - name: cache-volume
              mountPath: /opt/llama_cpp/cache
          startupProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 100
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
      volumes:
        - name: cache-volume
          persistentVolumeClaim:
            claimName: llama-cpp-cache-pvc
      nodeSelector:
        nvidia.com/gpu.product: Orin
---
apiVersion: v1
kind: Service
metadata:
  name: embeddinggemma-300m
  namespace: llm-serving
  labels:
    app: embeddinggemma-300m
spec:
  selector:
    app: embeddinggemma-300m
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    protocol: TCP
  type: ClusterIP
