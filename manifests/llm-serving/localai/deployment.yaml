apiVersion: apps/v1
kind: Deployment
metadata:
  name: localai-server
  labels:
    app: localai-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: localai-server
  template:
    metadata:
      labels:
        app: localai-server
    spec:
      nodeSelector:
        nvidia.com/gpu.product: Orin
      runtimeClassName: nvidia
      containers:
        - name: localai-server
          image: localai/localai:v3.3.1-nvidia-l4t-arm64
          args:
            - "federated"
            - "--log-level=debug"
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: LOCALAI_MODELS_PATH
              value: /localai/models
            - name: LOCALAI_BACKENDS_PATH
              value: /localai/backends
            - name: LOCALAI_P2P_LISTEN_MADDRS
              value: "/ip4/127.0.0.1/tcp/40207"
            - name: TOKEN
              valueFrom:
                secretKeyRef:
                  name: localai
                  key: TOKEN
          volumeMounts:
            - name: localai-volume
              mountPath: /localai
          ports:
            - name: http
              containerPort: 8080
            - name: p2p
              containerPort: 40207
      volumes:
        - name: localai-volume
          hostPath:
            path: /mnt/mfs/k8s/localai
            type: Directory
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: localai-worker
  labels:
    app: localai-worker
spec:
  serviceName: "localai-worker"
  replicas: 3
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: localai-worker
  template:
    metadata:
      labels:
        app: localai-worker
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - localai-worker
              topologyKey: "kubernetes.io/hostname"
      nodeSelector:
        nvidia.com/gpu.product: Orin
      runtimeClassName: nvidia
      containers:
        - name: localai-worker
          image: localai/localai:v3.3.1-nvidia-l4t-arm64
          args:
            - "run"
            - "--log-level=debug"
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: LOCALAI_MODELS_PATH
              value: /localai/models
            - name: LOCALAI_BACKENDS_PATH
              value: /localai/backends
            - name: LOCALAI_P2P
              value: "true"
            - name: LOCALAI_P2P_BOOTSTRAP_PEERS_MADDRS
              value: "/dns/localai-server/tcp/40207"
            - name: TOKEN
              valueFrom:
                secretKeyRef:
                  name: localai
                  key: TOKEN
          volumeMounts:
            - name: localai-volume
              mountPath: /localai
          ports:
            - name: grpc
              containerPort: 50051
      volumes:
        - name: localai-volume
          hostPath:
            path: /mnt/mfs/k8s/localai
            type: Directory
---
